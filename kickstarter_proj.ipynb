{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstarter project (need to spell check)\n",
    "## Our project for this semester is to try and predict whether a fundraising campaign in kickstarter will succeed or not.\n",
    "\n",
    "This type of prediction can actualy be useful in several scenarios, whether for an entrepreneur trying to evaluate his chances, the kickstarter company itself that would like to promote promising campaigns or for an investor considering backing a company.\n",
    "\n",
    "There are a few datasets available in kaggle such as: [here](https://www.kaggle.com/codename007/funding-successful-projects) and [here](https://www.kaggle.com/kemical/kickstarter-projects). These datasets are more limited timespan wise and in their richness of data. The dataset that we used in our project is offered [here](https://webrobots.io/kickstarter-datasets/). It is very large and somewhat messy, so our first steps are going to be devoted to get to know this dataset and clean it up so we can use it easily.\n",
    "\n",
    "The data is scraped over different periods, the last scrape is from Nov 2019 and contains 57 very large csv files. Our first step would be to unify it all (scrapes from 2015 onwards, each containing about 100,000 records, with a lot of overlaping) into a single dataframe, and explore the columns.\n",
    "Due to size limitations, we added an extra step here, and removed duplicates and live projects (which are about 10% of the data, but are usless). Otherwise, the built data frame might be to big to fit into memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import dataCleaning as dc\n",
    "import visio\n",
    "import inspect\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # some seaborn plots ommit warnings. Known issue.\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will auto download the cleaned dataset as a pickle and extract it. It is also possible to build the dataset yourself with passing the argument cache=None, but this is a lengthy process that might take a few hours (downloading about 50 generations of the dataset, each about 1GB and uniting them). Once this pickle is on your computer, it will be auto loaded from it's location.\n",
    "\n",
    "### note that this step requires internet connectivity and will download up to 1.5GB of data to your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dc.make_dataframe(path=r'rawData') #Files are assumed to be located in rawData sub.dir. caches pickle in cwd.\n",
    "#print first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's get a few details about this data: What are the features, how many records exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df.columns.values)\n",
    "print(cols)\n",
    "num_recs = len(df.index)\n",
    "print()\n",
    "print('There are originaly {} records in data'.format(num_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a first peek at the data via Excel hints that there are many empty columns:\n",
    "![peek](img/firstPeek.png)\n",
    "\n",
    "Let's see what columns contain mostly null values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nes = df.isna().sum()\n",
    "print(nes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not missing anything too important so far (though some sound important they are either not used or interchangable with other fields that are kept). Off with their head!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = {'friends','is_backing','is_starred','is_starrable','permissions','country_displayable_name','converted_pledged_amount',\n",
    "         'current_currency','usd_type','fx_rate', 'has_more','last_update_published_at','projects','search_url','seed','staff_pick','total_hits','unread_messages_count','unseen_activity_count'}\n",
    "letgo = [name for name in empty if name in cols]\n",
    "df.drop(columns=letgo,inplace=True)\n",
    "cols = list(df.columns.values)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see redundant attributes which we are sure we will not need:\n",
    "- Data that is used for display purpases: such as 'currency_symbol', 'currency_trailing_code'.\n",
    "- Data that is biased: such as backers count (This is part of the prediction), or disable_communication which is an option for failed projects \n",
    "- Data that will not be used by our model: location, 'profile', 'urls','usd_type', 'location'.\n",
    "Let's start with dropping these.\n",
    "\n",
    "Looks like we can drop 'friends','is_backing','is_starred','permissions' as they are basicaly empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant = {'backers_count','currency_symbol', 'currency_trailing_code','source_url','disable_communication',\n",
    "             'profile','urls','location','spotlight','usd_pledged'}\n",
    "letgo = [name for name in redundant if name in cols]\n",
    "df.drop(columns=letgo, inplace=True)\n",
    "cols = list(df.columns.values)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the data we can also see that the time fields are given in UNIX time. It'll be usefull ahead if we can break each date into a day month year trio. We'll run the conversion and replace each column with the corresponding 3 fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timefields = ['created_at','deadline','launched_at','state_changed_at']\n",
    "dc.convert_time(df,timefields)\n",
    "print('sanity check')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! looks alot better. now, one more check we need to do, is to check for duplicates in our dataset. If we find any duplicates (by project id). We will drop all earlier appearances of the same project. Note that this action sorts all projects by update date, so we need to take that in consideration up ahead. We created the dataset in a way which it won't include any duplicates, but just to be sure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were originaly {} records in data'.format(num_recs))\n",
    "dc.remove_duplicates(df)\n",
    "num_recs = len(df.index)\n",
    "print('After processing there are {} records in data'.format(num_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another inconviniency in this dataset is that some of the fields are given in json form, specificaly the 'catagory' and 'creator' attributs. We'll parse just the interesting parts out of these fields and remove all bloat text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dc.extract_creator(df) #replaces the creator json with creator id int, un\n",
    "df.drop(columns=['creator'], inplace=True) #currently not used.\n",
    "dc.extract_catagories(df) #gets project catagory data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing that remains is to convert the goal amount which is the project's local currency (and not usd).\n",
    "Once this is done we no longer need the static usd column (it is dropped by the function). We will also parse the project photo url for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.convert_goal(df)\n",
    "dc.get_image_url(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to begin exploring our data.\n",
    "As this is basicaly what we are asking, let's see how many projects of each status are in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_distriubtion_by_state_slice(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since live projects can't be used, we'll clear them out and also unite suspended and canceled project to be labled as failed. which gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.fix_state(df) #deletes live projects and unites failed.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.state.value_counts())\n",
    "visio.plot_distriubtion_by_state_slice(df)\n",
    "num_recs = len(df.index)\n",
    "print('After processing there are {} records in data'.format(num_recs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Looks like our data is balanced, and projects in our data set are eaqualy likely to fail or succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at how our data distributes globaly.\n",
    "projects by origin country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_by_country(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoose americans are always too big, let's try and give focus to the rest of the world as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_by_country(df.loc[df['country'] != 'US'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how success distributes by catagory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_by_category(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as product catagory has an impact on campaign result. Our data set allows us to view this in even finer granularity, by sub catagories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_by_sub_category(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to factor in is seasonality, let's see if there is any change in the success depending on project start month. To be able to look at this data over several years, we'll add specific month and year columns for launched_at and deadline. We will also add a field calculating the delta in months between launch and deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.extract_month_and_year(df, ['launched_at','deadline'])\n",
    "dc.add_destination_delta_in_days(df)\n",
    "visio.plot_success_by_launched_month(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, looking at the whole period of given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_over_time(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the duration of the campaign affects the probability of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_success_by_destination_delta_in_days(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = df.loc[df['goal']<30000]\n",
    "sns.distplot(inner['goal']).set(xlim=(0))\n",
    "print('number of records out of range:',len(df.loc[df['goal']>30000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = df.loc[df['goal']>30000]\n",
    "inner = inner.loc[df['goal']<200000]\n",
    "sns.distplot(inner['goal']).set(xlim=(0))\n",
    "#print('number of records out of range:',len(df.loc[df['goal']<50000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner = df.loc[df['goal']<80000]\n",
    "sns.distplot(inner['goal']).set(xlim=(0))\n",
    "print('number of records out of range:',len(df.loc[df['goal']>80000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent = df.loc[df['goal']<30000]\n",
    "cent.plot.scatter(x='goal',y='pledged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's try running a few naive models and see what it is that we are dealing with here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import knn_model as knn\n",
    "import logistic_regression_model as logistic\n",
    "import random_forest_model as forest\n",
    "import gradient_boosting_model as gradient_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_pr = logistic.run_model(df)\n",
    "models = {'Logistic regression' : logReg_pr}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few other models: KNN, Random forest and gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pr = knn.run_model(df)\n",
    "models['KNN'] = knn_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pr = forest.run_model(df)\n",
    "models['Random forest'] = forest_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_pr = gradient_boosting.run_model(df)\n",
    "models['Gradient boost'] = boost_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_precision(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So up until now we used standard techniques. Now we will try and leverage the most interesting data we have in out set. The free text fields (which are the project's name, and 'blurb' which is a short discription of the project), and the projects pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project photos\n",
    "The first thing we need to do to be able to gain some insights from the images is to be able to access them. We took a step in that direction, when parsing the urls for images in the dataset. Now the more chalenging part was to actualy obtain them. We chose to download them (as opposed to accessing them directly online or some other 'lazy' approach), as we predicted we would want to try a few different models on them and this would save us time on the long run. As it enabled us to run uninterupted and with faster connection we used a dedicated Azure cloud VM to download the 314K pictures weighing about 30GB. The whole downloading process took about 2 days (with the very naive and un-paralelised code bellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = inspect.getsource(dc.download_photos)\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we had the photos we needed to find what we can do with them (actualy we did the reaserch before opening a dedicated VM and dowloading, but this narrates better). \n",
    "\n",
    "Doing some reaserch, we found NIMA, a paper by google's AI team, that suggest's leveraging convolutional neural networks to predict how aesthetically pleasing a photograph is.\n",
    "\n",
    "https://arxiv.org/pdf/1709.05424.pdf\n",
    "\n",
    "This seemed like a novel feature and we decided to find an implementation of the model on-line, as no model was actualy released by google. We tried a few private repos on git-hub, which did not seem promising (running them on a small sub-sample gave results that did not sit well with our judgment of the photos).\n",
    "\n",
    "Finaly, we found a project by Idealo (a German e-commerce site, sort of like 'zap.co.il') which implements NIMA and was already succesfully used to rate hotels by on-line pictures.\n",
    "\n",
    "Leveraging the model on our dataset required some tweeking and learning, especially in the data loading phase, where the original input for the model was different than ours and so where the pictures formats). This was also quite chalenging as running the model was only possible using a docker container we needed to learn how to handle.\n",
    "\n",
    "Running the model on all 314K pictures with our GPU clad VM took several hours and yielded two jason arrays with the results. We can now add them into the dataset. As this is a lengthy process (due to the unfriendly output of the model) you can uncomment the cell bellow which will automatically download the clean dataset panda as a pickle and load it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dc.get_pickles('with_NIMA.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.add_nima(df, jsonFile='NIMA predictions/predictions_imgs_all.json', columnName = 'nima_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.add_nima(df, jsonFile='NIMA predictions/predictions_imgs_all_technical.json', columnName = 'nima_tech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nes = df.isna().sum()\n",
    "print(nes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['nima_score','nima_tech'], inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of what this model returned. We'll display bellow 9 random rhigh scoring images and 9 random low scoring ones. This function retreives these photos on-line, so it requires internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.display_imgs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the distribution of the technical ratings and the aesthetical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[['nima_score']], hist=False, rug=False, axlabel = 'Image score', label = 'aesthetic score')\n",
    "sns.distplot(df[['nima_tech']], hist=False, rug=False, label = 'Technical score').set_title('Image score distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = df.loc[df['state'] == 'successful']\n",
    "losers = df.loc[df['state'] == 'failed']\n",
    "sns.distplot(losers[['nima_score']], hist=False, rug=False, axlabel = 'Image score', label = 'failed projects')\n",
    "sns.distplot(winners[['nima_score']], hist=False, rug=False, label = 'successful projects').set_title('Image score distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(losers[['nima_tech']], hist=False, rug=False, axlabel = 'Image score', label = 'failed projects')\n",
    "sns.distplot(winners[['nima_tech']], hist=False, rug=False, label = 'successful projects').set_title('Image technical score distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the aesthetical model seems to be the one holding the most potential twards differentiating the distributions of the failed and successful projects, we will focus on it. Let's extract the distributions paramaters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mean = df.nima_score.mean()\n",
    "print('nima score total mean is {}'.format(total_mean))\n",
    "total_std = df.nima_score.std()\n",
    "print('nima score total std is {}'.format(total_std))\n",
    "winner_mean = winners.nima_score.mean()\n",
    "print('winners nima score mean is {}'.format(winner_mean))\n",
    "winner_std = winners.nima_score.std()\n",
    "print('winners nima score std is {}'.format(winner_std))\n",
    "loser_mean = losers.nima_score.mean()\n",
    "print('losers nime score mean is {}'.format(loser_mean))\n",
    "loser_std = losers.nima_score.std()\n",
    "print('losers nima score std is {}'.format(loser_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare general distribution to normal distribution with same mean and std\n",
    "sns.distplot(df[['nima_score']], hist=False, rug=False, axlabel = 'Image score', label = 'total aesthetic score')\n",
    "norm = np.random.normal(total_mean,total_std,300000)\n",
    "sns.distplot(norm, hist=False, rug=False, axlabel = 'Image score', label = 'normal distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare successful distribution to normal distribution with same mean and std\n",
    "sns.distplot(winners[['nima_score']], hist=False, rug=False, label = 'successful projects').set_title('Image score distribution')\n",
    "norm = np.random.normal(winner_mean,winner_std,300000)\n",
    "sns.distplot(norm, hist=False, rug=False, axlabel = 'Image score', label = 'normal distribution \\n with succ. params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare failed distribution to normal distribution with same mean and std\n",
    "sns.distplot(losers[['nima_score']], hist=False, rug=False, axlabel = 'Image score', label = 'failed projects')\n",
    "norm = np.random.normal(loser_mean,loser_std,300000)\n",
    "sns.distplot(norm, hist=False, rug=False, axlabel = 'Image score', label = 'normal distribution \\n with failed params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no ampirical normality test, but we can see that these distributions are practicaly normal, as is expected by the specification of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg_pr = logistic.run_model(df, nima = True)\n",
    "models['Logistic regression with nima'] = logReg_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pr = knn.run_model(df, nima = True)\n",
    "models['KNN with NIMA'] = knn_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pr = forest.run_model(df, nima = True)\n",
    "models['Random forest with nima'] = forest_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_pr = gradient_boosting.run_model(df, nima = True)\n",
    "models['Gradient boost with nima'] = boost_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visio.plot_precision(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
